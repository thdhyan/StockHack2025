{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import scipy.stats\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch as ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm1): LSTM(6, 256, num_layers=6, batch_first=True, bidirectional=True)\n",
      "  (lstm2): LSTM(256, 128, num_layers=6, batch_first=True, bidirectional=True)\n",
      "  (lstm3): LSTM(256, 128, num_layers=6, batch_first=True, bidirectional=True)\n",
      "  (lstm4): LSTM(256, 64, num_layers=6, batch_first=True, bidirectional=True)\n",
      "  (lstm5): LSTM(128, 64, num_layers=6, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# we are desinging a BI-LSTM model to predict the stock price of a company\n",
    "# we will use the past 60 days stock price to predict the next day stock price\n",
    "# we will use the stock price of ALT.csx for this task\n",
    " \n",
    "# load the data\n",
    "data = pandas.read_csv('data/ALT (1).csv')\n",
    "data = data.dropna()\n",
    "data = data[['open', 'high', 'low', 'close','volume','trade_count']]\n",
    "data = data.values\n",
    "\n",
    "# split the data into training and testing data\n",
    "train_data = data[:int(len(data)*0.8)]\n",
    "test_data = data[int(len(data)*0.8):]\n",
    "\n",
    "# scale the data\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "# create the training data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, len(train_data)):\n",
    "    X_train.append(train_data[i-60:i])\n",
    "    y_train.append(train_data[i])\n",
    "X_train = numpy.array(X_train)\n",
    "y_train = numpy.array(y_train)\n",
    "\n",
    "# create the testing data\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(60, len(test_data)):\n",
    "    X_test.append(test_data[i-60:i])\n",
    "    y_test.append(test_data[i])\n",
    "X_test = numpy.array(X_test)\n",
    "y_test = numpy.array(y_test)\n",
    "\n",
    "# create the model\n",
    "# Create a Bidirectional LSTM model with 5 Bi directional LSTM layers with 128, units for 3 layers and 2  layers with 64 units\n",
    "# the output layer will have 4 units\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size*2, num_layers-1, batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = ptr.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = ptr.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "device = ptr.device('cuda' if ptr.cuda.is_available() else 'cpu')\n",
    "model = LSTM(6, 128, 6, 4).to(device)\n",
    "\n",
    "model.compile()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10281/1642401872.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = ptr.tensor(X_train).float().to(device)\n",
      "/tmp/ipykernel_10281/1642401872.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = ptr.tensor(y_train).float().to(device)\n",
      "/tmp/ipykernel_10281/1642401872.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = ptr.tensor(X_test).float().to(device)\n",
      "/tmp/ipykernel_10281/1642401872.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = ptr.tensor(y_test).float().to(device)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] WON'T CONVERT forward /tmp/ipykernel_10281/809756035.py line 53 \n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] due to: \n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return _compile(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/usr/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 846, in _compile\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     raise InternalTorchDynamoError(str(e)).with_traceback(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     tracer.run()\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     super().run()\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     while self.step():\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1561, in LOAD_ATTR\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     self._load_attr(inst)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1551, in _load_attr\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     result = BuiltinVariable(getattr).call_function(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 962, in call_function\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return handler(tx, args, kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 846, in builtin_dipatch\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     rv = fn(tx, args, kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 764, in call_self_handler\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     result = self_handler(tx, *args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 1586, in call_getattr\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return obj.var_getattr(tx, name)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/nn_module.py\", line 285, in var_getattr\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     subobj = inspect.getattr_static(base, name)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/usr/lib/python3.8/inspect.py\", line 1622, in getattr_static\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     raise AttributeError(attr)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.InternalTorchDynamoError: lstm\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] \n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] from user code:\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]    File \"/tmp/ipykernel_10281/809756035.py\", line 56, in forward\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     out, _ = self.lstm(x, (h0, c0))\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] \n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] \n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] Traceback (most recent call last):\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 948, in __call__\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     result = self._inner_convert(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 472, in __call__\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return _compile(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_utils_internal.py\", line 84, in wrapper_function\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return StrobelightCompileTimeProfiler.profile_compile_time(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_strobelight/compile_time_profiler.py\", line 129, in profile_compile_time\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/usr/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return func(*args, **kwds)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 846, in _compile\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     raise InternalTorchDynamoError(str(e)).with_traceback(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 817, in _compile\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/utils.py\", line 231, in time_wrapper\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     r = func(*args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 636, in compile_inner\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     out_code = transform_code_object(code, transform)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1185, in transform_code_object\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     transformations(instructions, code_options)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 178, in _fn\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return fn(*args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/convert_frame.py\", line 582, in transform\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     tracer.run()\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 2451, in run\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     super().run()\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 893, in run\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     while self.step():\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 805, in step\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1561, in LOAD_ATTR\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     self._load_attr(inst)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/symbolic_convert.py\", line 1551, in _load_attr\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     result = BuiltinVariable(getattr).call_function(\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 962, in call_function\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return handler(tx, args, kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 846, in builtin_dipatch\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     rv = fn(tx, args, kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 764, in call_self_handler\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     result = self_handler(tx, *args, **kwargs)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/builtin.py\", line 1586, in call_getattr\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     return obj.var_getattr(tx, name)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/home/dhyan/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/variables/nn_module.py\", line 285, in var_getattr\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     subobj = inspect.getattr_static(base, name)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]   File \"/usr/lib/python3.8/inspect.py\", line 1622, in getattr_static\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     raise AttributeError(attr)\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] torch._dynamo.exc.InternalTorchDynamoError: lstm\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] \n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] from user code:\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]    File \"/tmp/ipykernel_10281/809756035.py\", line 56, in forward\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009]     out, _ = self.lstm(x, (h0, c0))\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] \n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0222 05:58:43.523450 140283555444544 torch/_dynamo/convert_frame.py:1009] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute 'lstm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n",
      "File \u001b[0;32m~/StockHack2025/sh25/lib/python3.8/site-packages/torch/nn/modules/module.py:1551\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1551\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compiled_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/StockHack2025/sh25/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:433\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    437\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/StockHack2025/sh25/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 56\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m h0 \u001b[38;5;241m=\u001b[39m ptr\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m c0 \u001b[38;5;241m=\u001b[39m ptr\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 56\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m(x, (h0, c0))\n\u001b[1;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/StockHack2025/sh25/lib/python3.8/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'lstm'"
     ]
    }
   ],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# train the model\n",
    "\n",
    "X_train = ptr.tensor(X_train).float().to(device)\n",
    "y_train = ptr.tensor(y_train).float().to(device)\n",
    "X_test = ptr.tensor(X_test).float().to(device)\n",
    "y_test = ptr.tensor(y_test).float().to(device)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    outputs = model(X_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch}: loss {loss.item()}')\n",
    "        \n",
    "# test the model\n",
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "loss = criterion(outputs, y_test)\n",
    "print(f'test loss {loss.item()}')\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(scaler.inverse_transform(y_test.cpu().detach().numpy()), label='True')\n",
    "plt.plot(scaler.inverse_transform(outputs.cpu().detach().numpy()), label='Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
